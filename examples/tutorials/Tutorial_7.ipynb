{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1715632748.588426] [d78f44b0045f:265496:f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from minerva.models.nets.unet import UNet\n",
    "from dasf.datasets import Dataset, DatasetArray\n",
    "from dasf.pipeline import Pipeline\n",
    "from dasf.pipeline.executors import DaskPipelineExecutor\n",
    "import lightning as L\n",
    "from dasf.ml.dl import LightningTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, original_path, label_path, chunks=(1, -1, -1)):\n",
    "        self.original = DatasetArray(\n",
    "            name=\"input\", root=original_path, chunks=chunks\n",
    "        )\n",
    "        self.label = DatasetArray(name=\"label\", root=label_path, chunks=chunks)\n",
    "\n",
    "    def load(self):\n",
    "        self.original.load()\n",
    "        self.label.load()\n",
    "        return self\n",
    "\n",
    "    def _lazy_load_cpu(self):\n",
    "        return self.load()\n",
    "\n",
    "    def _load_cpu(self):\n",
    "        return self.load()\n",
    "\n",
    "    def _lazy_load_gpu(self):\n",
    "        return self.load()\n",
    "\n",
    "    def _load_gpu(self):\n",
    "        return self.load()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.original[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 36811 instead\n",
      "  warnings.warn(\n",
      "2024-05-13 20:39:15,625 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fhewj3sb', purging\n",
      "2024-05-13 20:39:15,625 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r1ilx7aq', purging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-13 20:39:16+0000] INFO - Beginning pipeline run for 'pipeline'\n",
      "[2024-05-13 20:39:16+0000] INFO - Task 'LabeledDataset.load': Starting task run...\n",
      "[2024-05-13 20:39:16+0000] INFO - Task 'LabeledDataset.load': Finished task run\n",
      "[2024-05-13 20:39:16+0000] INFO - Task 'LightningTrainer.fit': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Missing logger folder: /workspaces/dasf/dasf-core/examples/tutorials/lightning_logs\n",
      "\n",
      "  | Name     | Type     | Params\n",
      "--------------------------------------\n",
      "0 | backbone | _UNet    | 31.0 M\n",
      "1 | fc       | Identity | 0     \n",
      "2 | loss_fn  | MSELoss  | 0     \n",
      "--------------------------------------\n",
      "31.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.0 M    Total params\n",
      "124.146   Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (16) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9aa815cac6a442f9c33236bc082e032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_path = \"/workspaces/dasf/data/original.npy\"\n",
    "label_path = \"/workspaces/dasf/data/envelope.npy\"\n",
    "\n",
    "model = UNet()\n",
    "dataset = LabeledDataset(original_path, label_path)\n",
    "trainer = LightningTrainer(\n",
    "    model=model,\n",
    "    use_gpu=False,\n",
    "    max_epochs=1,\n",
    "    limit_train_batches=16,\n",
    "    unsqueeze_dim=0,\n",
    "    strategy=\"ddp_notebook\",    # Only for jupyter notebook\n",
    "    devices=1,                  # Only for jupyter notebook\n",
    ")\n",
    "\n",
    "executor = DaskPipelineExecutor(\n",
    "    local=True, use_gpu=False\n",
    ")\n",
    "pipeline = Pipeline(\n",
    "    name=\"pipeline\",\n",
    "    executor=executor,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "pipeline.add(trainer.fit, train_data=dataset)\n",
    "\n",
    "pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
